{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":13129,"status":"ok","timestamp":1734379504123,"user":{"displayName":"Onur Cay","userId":"09681506537499586844"},"user_tz":0},"id":"EwaM5JWoD_j7","outputId":"6af40c70-ad79-408c-e5f7-e25894e7db4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.2/512.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q flwr[simulation] flwr-datasets[vision] torch torchvision matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16485,"status":"ok","timestamp":1734380259716,"user":{"displayName":"Onur Cay","userId":"09681506537499586844"},"user_tz":0},"id":"yxTARG2lMoOI","outputId":"5ce78fab-18b8-437f-9b0c-ce7480e55032"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training on cpu\n","Flower 1.13.1 / PyTorch 2.5.1+cu121\n"]}],"source":["from collections import OrderedDict\n","from typing import List, Tuple\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torchvision import transforms, datasets, models\n","from datasets.utils.logging import disable_progress_bar\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","\n","import flwr\n","from flwr.client import Client, ClientApp, NumPyClient\n","from flwr.common import Metrics, Context\n","from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n","from flwr.server.strategy import FedAvg\n","from flwr.simulation import run_simulation\n","from flwr_datasets import FederatedDataset\n","\n","DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n","print(f\"Training on {DEVICE}\")\n","print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n","disable_progress_bar()\n"]},{"cell_type":"markdown","metadata":{"id":"IbikFC8NFnAk"},"source":["This the same dataset. I will be tweaking some settings in order to get better accuracy. Currently I am getting 43% I aim for 90%+"]},{"cell_type":"markdown","metadata":{"id":"1D7VegnOHkWy"},"source":["I will be using a pretrained resnet model. I have edited the final layer so its 10 labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9YJLirdOnK7"},"outputs":[],"source":["class CIFAR10DictWrapper(Dataset):\n","    \"\"\"\n","    A wrapper around the CIFAR-10 dataset to convert output to dictionary format.\n","    \"\"\"\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        img, label = self.dataset[idx]\n","        return {\"img\": img, \"label\": label}\n","\n","def load_cifar10_centralized(batch_size=128):\n","    \"\"\"\n","    Load CIFAR-10 dataset in centralized mode and return DataLoaders with dictionary key format.\n","\n","    Args:\n","        batch_size (int): Batch size for DataLoader.\n","\n","    Returns:\n","        Tuple[DataLoader, DataLoader]: Train and test DataLoaders.\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n","    ])\n","\n","    # Load CIFAR-10 datasets\n","    trainset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n","    testset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n","\n","    # Wrap the datasets to return dictionary format\n","    trainset = CIFAR10DictWrapper(trainset)\n","    testset = CIFAR10DictWrapper(testset)\n","\n","    # Create DataLoaders\n","    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n","\n","    return trainloader, testloader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pjxCkuVdXlI"},"outputs":[],"source":["NUM_CLIENTS=10\n","BATCH_SIZE=64\n","\n","def load_datasets(partition_id: int):\n","    \"\"\"\n","    Load datasets for both centralized and federated learning.\n","\n","    Args:\n","        partition_id (int):\n","            For federated learning, this specifies the client partition ID.\n","            For centralized learning, set partition_id=0 to use the full dataset.\n","\n","    Returns:\n","        Tuple[DataLoader, DataLoader, DataLoader]: Train, validation, and test loaders.\n","    \"\"\"\n","    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": NUM_CLIENTS})\n","    partition = fds.load_partition(partition_id)\n","    # Divide data into 80% train, 20% validation for each partition\n","    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n","\n","    pytorch_transforms = transforms.Compose([\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n","    ])\n","\n","    def apply_transforms(batch):\n","        # Apply transforms to the \"img\" key in the batch\n","        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n","        return batch\n","\n","    # Apply transforms and create DataLoaders\n","    partition_train_test = partition_train_test.with_transform(apply_transforms)\n","    trainloader = DataLoader(\n","        partition_train_test[\"train\"], batch_size=BATCH_SIZE, shuffle=True\n","    )\n","    valloader = DataLoader(partition_train_test[\"test\"], batch_size=BATCH_SIZE)\n","    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n","    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n","\n","    return trainloader, valloader, testloader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNB-5QOLRNoi"},"outputs":[],"source":["class ResNetCIFAR10(nn.Module):\n","    def __init__(self):\n","        super(ResNetCIFAR10, self).__init__()\n","        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 10)\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n"]},{"cell_type":"markdown","metadata":{"id":"iBGG6xkw-b5Y"},"source":["We initialize the cnn as it works great with images\n","\n","We initialize the parent class of Net which is nn.Module.  \n","Then we start registering our layers. First layer in a CNN is often a convolutional layer. This layer usually has 3 inputs since there are 3 rgb channels for images. This layer applies a filter to find the edges, textures or objects in the image. We also define the output as 6 feature maps. And finally we apply a 5x5 filter to do the filtering.\n","\n","We then have a pooling layer which is a maximum pooling layer. This layer essentially takes the max value from a 2x2 region meaning, keeps the important features and discards the rest while retaining feature map count.\n","\n","We then have another conv layer does the same thing but now there are 16 feature maps. Helps extract more complex features from the pooling layer.\n","\n","We then have our first fully connected layer. Our input is the flattened features from the convolutional layers. 16 feature maps then 5x5 for the size of the features. We then output 120 neurons\n","\n","Second FC layer. takes 120 inputs gives 84 outputs. Processes higher level features and \"compresses\" the model making it more dense.\n","\n","Final layer takes those 84 features as input and produces 10 outputs which are the labels from the CIFAR-10 dataset.\n","\n","\n","We then have the forward pass defined. x is passed in which is the image data.\n","We then apply the mentioned layers so conv layer then relu actv func is applied. Then we apply the pool layer. This reduces the computational load making the model more efficent.  \n","\n","The same process is than repeated with the conv2 layer.\n","We than flatten to a 1D vector to prepare for the fully connected layer.\n","\n","Then we pass the image data thru the first fully connected layer andthen use ReLu activation. Repeat for the next step just making it more denser\n","We than have the last layer where we get the 10 labels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIpsXovcRNvU"},"outputs":[],"source":["\n","def train(net, trainloader, epochs, lr=0.1):\n","    \"\"\"Train the network.\"\"\"\n","    criterion = nn.CrossEntropyLoss()\n","    #optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","    optimizer = torch.optim.Adam(net.parameters())\n","\n","    net.train()\n","\n","\n","    for epoch in range(epochs):\n","        running_loss, correct, total = 0.0, 0, 0\n","        for batch in trainloader:\n","            images, labels = batch[\"img\"].to(DEVICE), batch[\"label\"].to(DEVICE)  # Access dictionary keys\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        #scheduler.step()\n","        print(f\"Epoch {epoch+1}/{epochs}: Loss={running_loss/len(trainloader):.4f}, Accuracy={100*correct/total:.2f}%\")\n","\n","\n","\n","def test(net, testloader):\n","    \"\"\"Evaluate the network on the entire test set.\"\"\"\n","    criterion = torch.nn.CrossEntropyLoss()\n","    correct, total, loss = 0, 0, 0.0\n","    net.eval()\n","    with torch.no_grad():\n","        for batch in testloader:\n","            images, labels = batch[\"img\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n","            outputs = net(images)\n","            loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    loss /= len(testloader.dataset)\n","    accuracy = correct / total\n","    return loss, accuracy\n"]},{"cell_type":"markdown","metadata":{"id":"vDhhB2x-RNMG"},"source":["The train function defines the optimizer and the loss function to be used in the training program\n","\n","Then we put the model in to training mode in line 6\n","\n","Then we run the epochs for the specified amount.\n","We load the batches and move them to the specified device in this case gpu\n","\n","We zero the gradient to avoid accumulation.\n","\n","We perform a forward pass line 12 gathers the predicted outputs,then next line we calculate the loss giving it the predicted and the true labels.\n","\n","Then we do a backward pass and run the optimizer which is adam in this case.\n","\n","We gather out loss for accuracy calculation then for verbose we have the verbose print statement.\n","\n","for he test function we are using the same loss func. then we set the model in eval mode.\n","\n","disable gradient tracking to save memory and speed up computation\n","\n","we itirate over the batches in tetloader. then we do a foward pass and compute the loss.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cC_ITZOQTe5T"},"source":["BELOW CELL\n","\n","we load the data then crate a CNN instance.\n","\n","we than have the training loop for 5 epochs\n","we evaluate the model using test defined above\n","\n","and we do final test to evalute the model once the epochs are over.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"elapsed":9929,"status":"error","timestamp":1734379566851,"user":{"displayName":"Onur Cay","userId":"09681506537499586844"},"user_tz":0},"id":"x9y2u-4iOwuz","outputId":"c31e5891-19a5-4c75-d711-0a28b0e3aa37"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170M/170M [00:03\u003c00:00, 47.7MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00\u003c00:00, 97.7MB/s]\n"]},{"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-0a3152ccf8c5\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar10_centralized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-\u003e 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["trainloader, testloader = load_cifar10_centralized()\n","net = ResNetCIFAR10().to(DEVICE)\n","train(net, trainloader, epochs=20, lr=0.1)\n","test(net, testloader)\n"]},{"cell_type":"markdown","metadata":{"id":"rF1IBbsdT85K"},"source":["Set and get paramters used by the FL algo to get weights and biases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJ52AIofRN-n"},"outputs":[],"source":["def set_parameters(net, parameters: List[np.ndarray]):\n","    params_dict = zip(net.state_dict().keys(), parameters)\n","    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n","    net.load_state_dict(state_dict, strict=True)\n","\n","\n","def get_parameters(net) -\u003e List[np.ndarray]:\n","    return [val.cpu().numpy() for _, val in net.state_dict().items()]"]},{"cell_type":"markdown","metadata":{"id":"6lH4f9e7UVKx"},"source":["We implement the client here extending the flwr.client.NumPyClient\n","\n","we init the client with local instances of the neural network,\n","the train data and validation data set are also loaded. In a real environment client would load its own dataset and its own model instance.\n","\n","get parameters send the clients current model parameters to the server. This funtion is used by the server to get the model paramters for aggregation.\n","\n","the fit function received the global params and trains the model locally on client dataset.\n","\n","the evaluate function evaluates the model using the validation dataset and sends the results to the server"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KpcOiaKROGh"},"outputs":[],"source":["class FederatedClient(flwr.client.NumPyClient):\n","    def __init__(self, net, trainloader, testloader):\n","        self.net = net\n","        self.trainloader = trainloader\n","        self.testloader = testloader\n","\n","    def get_parameters(self, config):\n","        return [val.cpu().numpy() for val in self.net.state_dict().values()]\n","\n","    def fit(self, parameters, config):\n","      self.set_parameters(parameters)\n","      lr = config[\"lr\"]\n","      local_epochs = config[\"local_epochs\"]\n","      train(self.net, self.trainloader, epochs=local_epochs, lr=lr)\n","      return self.get_parameters(config), len(self.trainloader.dataset), {}\n","\n","\n","    def evaluate(self, parameters, config):\n","        self.set_parameters(parameters)\n","        loss, accuracy = test(self.net, self.testloader)\n","        return float(loss), len(self.testloader.dataset), {\"accuracy\": float(accuracy)}\n","\n","    def set_parameters(self, parameters):\n","        params_dict = zip(self.net.state_dict().keys(), parameters)\n","        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n","        self.net.load_state_dict(state_dict, strict=True)\n"]},{"cell_type":"markdown","metadata":{"id":"zdsjZwSOo7t_"},"source":["This is the function to crate the client.\n","We specify device type cpu or gpu\n","load the data\n","then return the client instance\n","client app is used to managed the clients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhi3ZFj-bAwz"},"outputs":[],"source":["def client_fn(context: Context) -\u003e Client:\n","    \"\"\"Create a Flower client representing a single organization.\"\"\"\n","\n","    # Load model\n","    net = ResNetCIFAR10().to(DEVICE)\n","\n","    # Load data (CIFAR-10)\n","    # Note: each client gets a different trainloader/valloader, so each client\n","    # will train and evaluate on their own unique data partition\n","    # Read the node_config to fetch data partition associated to this node\n","    partition_id = context.node_config[\"partition-id\"]\n","    trainloader, valloader, testloader=load_datasets(partition_id=partition_id)\n","\n","    # Create a single Flower client representing a single organization\n","    # FlowerClient is a subclass of NumPyClient, so we need to call .to_client()\n","    # to convert it to a subclass of `flwr.client.Client`\n","    return FederatedClient(net, trainloader, valloader).to_client()\n","\n","\n","# Create the ClientApp\n","client = ClientApp(client_fn=client_fn)\n","\n","#For creating clients."]},{"cell_type":"markdown","metadata":{"id":"J6BGrYUBpnTO"},"source":["We define the strat that we are going to be using here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lEBbBGIbA30"},"outputs":[],"source":["# Create FedAvg strategy\n","strategy = FedAvg(\n","    fraction_fit=1.0,  # Sample 100% of available clients for training\n","    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n","    min_fit_clients=10,  # Never sample less than 10 clients for training\n","    min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n","    min_available_clients=10,  # Wait until all 10 clients are available\n","    on_fit_config_fn=lambda round: {\"lr\": 0.1, \"local_epochs\": 1},  # Pass to clients\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XelWpstJbA-r"},"outputs":[],"source":["def server_fn(context: Context) -\u003e ServerAppComponents:\n","    \"\"\"Construct components that set the ServerApp behaviour.\n","\n","    You can use the settings in `context.run_config` to parameterize the\n","    construction of all elements (e.g the strategy or the number of rounds)\n","    wrapped in the returned ServerAppComponents object.\n","    \"\"\"\n","\n","\n","    config = ServerConfig(num_rounds=20)\n","\n","    return ServerAppComponents(strategy=strategy, config=config)\n","\n","\n","# Create the ServerApp\n","server = ServerApp(server_fn=server_fn)"]},{"cell_type":"markdown","metadata":{"id":"oow5Rhp9p5Ye"},"source":["we define our server on the code above. Set the number of rounds we want to do.\n","then we just use serverappcomponents to initilize the server\n","\n","BELOW CELL\n","we give the resources we have depending on device type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrs1yVUHbBFD"},"outputs":[],"source":["# Specify the resources each of your clients need\n","# By default, each client will be allocated 1x CPU and 0x GPUs\n","backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n","\n","# When running on GPU, assign an entire GPU for each client\n","if DEVICE.type == \"cuda\":\n","    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n","    # Refer to our Flower framework documentation for more details about Flower simulations\n","    # and how to set up the `backend_config`"]},{"cell_type":"markdown","metadata":{"id":"FXQT2Ih1r_cy"},"source":["Here we run the simulation using the defined server and client app, number of clients and our config."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":283256,"status":"error","timestamp":1734373841216,"user":{"displayName":"Onur Cay","userId":"09681506537499586844"},"user_tz":0},"id":"fKk2dcXXbGb1","outputId":"27b9fdb3-5860-4dbd-bbeb-6377f5dcb8b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [INIT]\n","\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n","\u001b[36m(pid=49144)\u001b[0m 2024-12-16 18:26:10.296706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(pid=49144)\u001b[0m 2024-12-16 18:26:10.338256: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(pid=49144)\u001b[0m 2024-12-16 18:26:10.348979: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(pid=49144)\u001b[0m 2024-12-16 18:26:12.081306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m given by the platformdirs library.  To remove this warning and\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m see the appropriate new directories, set the environment variable\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m The use of platformdirs will be the default in `jupyter_core` v6\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n","\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n","\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n","\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [ROUND 1]\n","\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=6.5968, Accuracy=11.00%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=4.7521, Accuracy=12.97%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=6.4329, Accuracy=12.43%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=4.7578, Accuracy=13.38%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=5.2113, Accuracy=12.78%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=6.2784, Accuracy=11.47%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=5.8104, Accuracy=11.88%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=5.9332, Accuracy=11.15%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=4.7600, Accuracy=12.18%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=5.4758, Accuracy=10.93%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n","\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n","\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n","\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n","\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [ROUND 2]\n","\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=4.0726, Accuracy=10.60%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m Epoch 1/1: Loss=4.0501, Accuracy=11.72%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","\u001b[36m(ClientAppActor pid=49144)\u001b[0m   warnings.warn(msg)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/run_simulation.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(num_supernodes, backend_name, backend_config_stream, app_dir, is_app, enable_tf_gpu_growth, run, exit_event, delay_start, flwr_dir, client_app, client_app_attr, server_app, server_app_attr, server_app_run_config)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# Start Simulation Engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 360\u001b[0;31m         vce.start_vce(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mnum_supernodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_supernodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/superlink/fleet/vce/vce_api.py\u001b[0m in \u001b[0;36mstart_vce\u001b[0;34m(backend_name, backend_config_json_stream, app_dir, is_app, f_stop, run, flwr_dir, client_app, client_app_attr, num_supernodes, state_factory, existing_nodes_mapping)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Run main simulation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 374\u001b[0;31m         run_api(\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mapp_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/superlink/fleet/vce/vce_api.py\u001b[0m in \u001b[0;36mrun_api\u001b[0;34m(app_fn, backend_fn, nodes_mapping, state_factory, node_info_stores, f_stop)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 220\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             _ = [\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-21-07ca1bc7e62c\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m run_simulation(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mserver_app\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclient_app\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_supernodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/run_simulation.py\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m(server_app, client_app, num_supernodes, backend_name, backend_config, enable_tf_gpu_growth, verbose_logging)\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 210\u001b[0;31m     _run_simulation(\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mnum_supernodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_supernodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mclient_app\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_app\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/run_simulation.py\u001b[0m in \u001b[0;36m_run_simulation\u001b[0;34m(num_supernodes, exit_event, client_app, server_app, backend_name, backend_config, client_app_attr, server_app_attr, server_app_run_config, app_dir, flwr_dir, run, enable_tf_gpu_growth, delay_start, verbose_logging, is_app)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_logger_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 490\u001b[0;31m         \u001b[0m_main_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/run_simulation.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(num_supernodes, backend_name, backend_config_stream, app_dir, is_app, enable_tf_gpu_growth, run, exit_event, delay_start, flwr_dir, client_app, client_app_attr, server_app, server_app_attr, server_app_run_config)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"success\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mserverapp_th\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 385\u001b[0;31m             \u001b[0mserverapp_th\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mserver_app_thread_has_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exception in ServerApp thread\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Run simulation\n","run_simulation(\n","    server_app=server,\n","    client_app=client,\n","    num_supernodes=10,\n","    backend_config=backend_config,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnZjo_X7bIiY"},"outputs":[],"source":["def weighted_average(metrics: List[Tuple[int, Metrics]]) -\u003e Metrics:\n","    # Multiply accuracy of each client by number of examples used\n","    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n","    examples = [num_examples for num_examples, _ in metrics]\n","\n","    # Aggregate and return custom metric (weighted average)\n","    return {\"accuracy\": sum(accuracies) / sum(examples)}"]},{"cell_type":"markdown","metadata":{"id":"ir1wPCGlsL49"},"source":["same process as before just this time we add the weigted avarage function to see our accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bDziFUOTbIpD"},"outputs":[{"name":"stderr","output_type":"stream","text":["DEBUG:flwr:Asyncio event loop already running.\n","\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=10, no round_timeout\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [INIT]\n","\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n","\u001b[36m(pid=4471)\u001b[0m 2024-12-16 20:18:33.326830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(pid=4471)\u001b[0m 2024-12-16 20:18:33.373302: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(pid=4471)\u001b[0m 2024-12-16 20:18:33.391623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(pid=4471)\u001b[0m 2024-12-16 20:18:36.204951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m /usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m given by the platformdirs library.  To remove this warning and\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m see the appropriate new directories, set the environment variable\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m The use of platformdirs will be the default in `jupyter_core` v6\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m   from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n","\u001b[36m(pid=4470)\u001b[0m 2024-12-16 20:18:33.321670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","\u001b[36m(pid=4470)\u001b[0m 2024-12-16 20:18:33.381569: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","\u001b[36m(pid=4470)\u001b[0m 2024-12-16 20:18:33.397634: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[36m(pid=4470)\u001b[0m 2024-12-16 20:18:36.279721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n","\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n","\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [ROUND 1]\n","\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m /usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m given by the platformdirs library.  To remove this warning and\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m see the appropriate new directories, set the environment variable\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m The use of platformdirs will be the default in `jupyter_core` v6\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m   from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.7533, Accuracy=37.88%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.8068, Accuracy=36.05%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.3144, Accuracy=55.40%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.3286, Accuracy=53.27%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=1.1312, Accuracy=60.40%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=1.0079, Accuracy=65.83%\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=1.0564, Accuracy=64.05%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.9145, Accuracy=69.17%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.9838, Accuracy=66.83%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.7061, Accuracy=40.02%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.7494, Accuracy=38.73%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.2915, Accuracy=56.12%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.2714, Accuracy=55.10%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=1.1216, Accuracy=60.92%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=1.1184, Accuracy=62.00%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=1.0143, Accuracy=65.72%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=1.0325, Accuracy=64.40%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.9434, Accuracy=67.88%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.9489, Accuracy=68.05%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.8022, Accuracy=37.55%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.8200, Accuracy=36.50%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.3239, Accuracy=53.27%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.3019, Accuracy=54.55%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=1.1678, Accuracy=61.27%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=1.1707, Accuracy=59.62%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=1.0817, Accuracy=62.65%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=1.0739, Accuracy=63.02%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.9735, Accuracy=67.40%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.9963, Accuracy=66.50%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.8529, Accuracy=34.92%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.7850, Accuracy=36.45%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.3695, Accuracy=52.27%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.3536, Accuracy=52.95%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=1.2206, Accuracy=58.73%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=1.1509, Accuracy=60.38%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=1.1222, Accuracy=62.25%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=1.0418, Accuracy=62.98%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.9738, Accuracy=66.70%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.9549, Accuracy=67.40%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.7892, Accuracy=37.70%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.7735, Accuracy=38.08%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.3028, Accuracy=55.55%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.3414, Accuracy=52.58%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=1.1461, Accuracy=61.45%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=1.1310, Accuracy=60.83%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=1.0431, Accuracy=64.80%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=1.0535, Accuracy=63.67%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=1.0126, Accuracy=66.33%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.9889, Accuracy=66.72%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n","\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n","\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n","\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [ROUND 2]\n","\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.1728, Accuracy=60.02%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.0234, Accuracy=66.58%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.0308, Accuracy=66.12%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.9249, Accuracy=69.92%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.9023, Accuracy=69.75%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=0.8611, Accuracy=71.22%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=0.8893, Accuracy=70.33%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.7929, Accuracy=73.67%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.8226, Accuracy=72.45%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.1693, Accuracy=61.12%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.1758, Accuracy=60.58%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=1.0229, Accuracy=66.67%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.0256, Accuracy=65.53%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.9769, Accuracy=67.50%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.9453, Accuracy=68.33%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=0.9170, Accuracy=70.30%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=0.8410, Accuracy=72.62%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.8107, Accuracy=73.25%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.8227, Accuracy=72.25%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.1492, Accuracy=60.98%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=1.1767, Accuracy=60.02%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=0.9868, Accuracy=67.72%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.0014, Accuracy=66.88%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.9300, Accuracy=68.62%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=0.8330, Accuracy=71.38%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.7980, Accuracy=73.33%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.1536, Accuracy=60.67%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=0.9731, Accuracy=66.95%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.9249, Accuracy=68.95%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=0.8352, Accuracy=71.95%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.7914, Accuracy=73.80%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.8257, Accuracy=72.17%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=1.1542, Accuracy=61.62%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=1.0137, Accuracy=66.85%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.9577, Accuracy=67.67%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.9343, Accuracy=69.72%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=0.8953, Accuracy=69.75%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=0.8738, Accuracy=70.90%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.8152, Accuracy=72.08%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.8731, Accuracy=71.10%\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[92mINFO \u001b[0m:      aggregate_fit: received 10 results and 0 failures\n","\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n","\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n","\u001b[92mINFO \u001b[0m:      \n","\u001b[92mINFO \u001b[0m:      [ROUND 3]\n","\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=0.9769, Accuracy=68.33%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=0.8411, Accuracy=72.05%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.7868, Accuracy=74.45%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=0.7414, Accuracy=75.40%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.6779, Accuracy=76.90%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=0.9536, Accuracy=68.72%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=0.8317, Accuracy=71.92%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.7746, Accuracy=73.70%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=0.7358, Accuracy=75.20%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.7059, Accuracy=76.20%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=0.9029, Accuracy=69.40%\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=0.9466, Accuracy=68.45%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=0.8297, Accuracy=72.00%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=0.8459, Accuracy=72.38%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.7795, Accuracy=73.65%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.7876, Accuracy=73.88%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 4/5: Loss=0.7401, Accuracy=75.12%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 4/5: Loss=0.7378, Accuracy=76.30%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 5/5: Loss=0.6573, Accuracy=77.95%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 5/5: Loss=0.7426, Accuracy=75.47%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 1/5: Loss=0.9300, Accuracy=69.72%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 1/5: Loss=0.9325, Accuracy=69.62%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 2/5: Loss=0.8051, Accuracy=73.65%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 2/5: Loss=0.8291, Accuracy=71.95%\n","\u001b[36m(ClientAppActor pid=4471)\u001b[0m Epoch 3/5: Loss=0.7864, Accuracy=74.20%\n","\u001b[36m(ClientAppActor pid=4470)\u001b[0m Epoch 3/5: Loss=0.7820, Accuracy=73.85%\n"]}],"source":["def server_fn(context: Context) -\u003e ServerAppComponents:\n","    \"\"\"Construct components that set the ServerApp behaviour.\n","\n","    You can use settings in `context.run_config` to parameterize the\n","    construction of all elements (e.g the strategy or the number of rounds)\n","    wrapped in the returned ServerAppComponents object.\n","    \"\"\"\n","\n","    # Create FedAvg strategy\n","    strategy = FedAvg(\n","        fraction_fit=1.0,\n","        fraction_evaluate=0.5,\n","        min_fit_clients=10,\n","        min_evaluate_clients=5,\n","        min_available_clients=10,\n","        on_fit_config_fn=lambda round: {\"lr\": 0.1, \"local_epochs\": 5},  # Pass to clients\n","        evaluate_metrics_aggregation_fn=weighted_average,  # \u003c-- pass the metric aggregation function\n","    )\n","\n","    # Configure the server for 5 rounds of training\n","    config = ServerConfig(num_rounds=10)\n","\n","    return ServerAppComponents(strategy=strategy, config=config)\n","\n","\n","# Create a new server instance with the updated FedAvg strategy\n","server = ServerApp(server_fn=server_fn)\n","\n","# Run simulation\n","run_simulation(\n","    server_app=server,\n","    client_app=client,\n","    num_supernodes=10,\n","    backend_config=backend_config,\n",")"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyNyBSRo49D8DPTcluvytibC","gpuType":"V28","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}